{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "* The purpose of this kernel is to convert the collection of 684 job bulletin text files into a single structured .csv file (aka flat file)\n",
    "* A job bulletin has a hierarchical structure: within the Carpenter bulletin, the job titles \"carpenter\" and \"cabinetmaker\" are within the apprenticeship experience requirement; another branch of the tree only requires full time paid experience doing carpenter or cabinetmaker work\n",
    "* I think that this is the fundamental problem: in a csv, each leaf of the tree should either be on its own line (if it has parents, e.g. a sub-requirement) or in its own column (if it has no parents, e.g. duties or salary). \n",
    "* Getting this right will let us feed the hierarchical structure of requirements directly into a graph of promotions\n",
    "* If this csv format is followed, then every line in the csv should correspond to a path in the promotion graph\n",
    "* My strategy is to use regular expressions to tokenize the entries in the columns of the structured csv\n",
    "* To help with this, I've taken some vocabulary from Los Angeles area college course catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_execution_state": "idle",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "051d70d956493feee0c6d64651c6a088724dca2a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "── \u001b[1mAttaching packages\u001b[22m ─────────────────────────────────────── tidyverse 1.2.1 ──\n",
      "\u001b[32m✔\u001b[39m \u001b[34mggplot2\u001b[39m 3.1.1.\u001b[31m9000\u001b[39m     \u001b[32m✔\u001b[39m \u001b[34mpurrr  \u001b[39m 0.3.2     \n",
      "\u001b[32m✔\u001b[39m \u001b[34mtibble \u001b[39m 2.1.1          \u001b[32m✔\u001b[39m \u001b[34mdplyr  \u001b[39m 0.8.1     \n",
      "\u001b[32m✔\u001b[39m \u001b[34mtidyr  \u001b[39m 0.8.3          \u001b[32m✔\u001b[39m \u001b[34mstringr\u001b[39m 1.4.0     \n",
      "\u001b[32m✔\u001b[39m \u001b[34mreadr  \u001b[39m 1.3.1          \u001b[32m✔\u001b[39m \u001b[34mforcats\u001b[39m 0.4.0     \n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\n",
      "Attaching package: ‘recipes’\n",
      "\n",
      "The following object is masked from ‘package:stringr’:\n",
      "\n",
      "    fixed\n",
      "\n",
      "The following object is masked from ‘package:stats’:\n",
      "\n",
      "    step\n",
      "\n",
      "Package version: 1.4.3\n",
      "Parallel computing: 2 of 4 threads used.\n",
      "See https://quanteda.io for tutorials and examples.\n",
      "\n",
      "Attaching package: ‘quanteda’\n",
      "\n",
      "The following object is masked from ‘jupyter:irkernel’:\n",
      "\n",
      "    View\n",
      "\n",
      "The following object is masked from ‘package:utils’:\n",
      "\n",
      "    View\n",
      "\n",
      "\n",
      "Attaching package: ‘igraph’\n",
      "\n",
      "The following object is masked from ‘package:quanteda’:\n",
      "\n",
      "    as.igraph\n",
      "\n",
      "The following objects are masked from ‘package:dplyr’:\n",
      "\n",
      "    as_data_frame, groups, union\n",
      "\n",
      "The following objects are masked from ‘package:purrr’:\n",
      "\n",
      "    compose, simplify\n",
      "\n",
      "The following object is masked from ‘package:tidyr’:\n",
      "\n",
      "    crossing\n",
      "\n",
      "The following object is masked from ‘package:tibble’:\n",
      "\n",
      "    as_data_frame\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    decompose, spectrum\n",
      "\n",
      "The following object is masked from ‘package:base’:\n",
      "\n",
      "    union\n",
      "\n",
      "Loading required package: xml2\n",
      "\n",
      "Attaching package: ‘rvest’\n",
      "\n",
      "The following object is masked from ‘package:purrr’:\n",
      "\n",
      "    pluck\n",
      "\n",
      "The following object is masked from ‘package:readr’:\n",
      "\n",
      "    guess_encoding\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Importing packages\n",
    "\n",
    "library(tidyverse)\n",
    "library(tidytext)\n",
    "library(recipes)\n",
    "library(quanteda)\n",
    "#library(hunspell)\n",
    "library(igraph)\n",
    "#library(pdftools)\n",
    "library(rvest)\n",
    "\n",
    "## Set seed \n",
    "set.seed(79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsed with column specification:\n",
      "cols(\n",
      "  title = \u001b[31mcol_character()\u001b[39m\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# List files for input\n",
    "file_list  <- list.files('../input/data-science-for-good-city-of-los-angeles/cityofla//CityofLA//Job Bulletins',\n",
    "                        full.names = TRUE)\n",
    "job_titles <- read_csv('../input/data-science-for-good-city-of-los-angeles/cityofla/CityofLA/Additional data/job_titles.csv', col_names = \"title\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build keyword dictionaries\n",
    "# Remove stop words \n",
    "\n",
    "job_words <- job_titles$title %>% \n",
    "  str_split(\"_\") %>% \n",
    "  unlist %>% \n",
    "  str_to_lower %>%\n",
    "  unique %>% \n",
    "  str_squish() %>%\n",
    "  as.character() %>% \n",
    "  str_remove_all(pattern = '\\\\\\\\\"') %>% \n",
    "  str_remove_all(pattern=\"(of|and|code:|only|1968|\\\\(|\\\\)|to|2315)\") %>% \n",
    "  unique\n",
    "\n",
    "job_words <- job_words[length(job_words > 0)]\n",
    " job_words <- job_words[str_length(job_words) > 1]\n",
    "\n",
    "job_dictionary <- dictionary(list(job_title = job_words))\n",
    "\n",
    "elac_programs <- read_file(\"../input/elac-programs/elac_terms.txt\")\n",
    "\n",
    "\n",
    "ucla_programs <-\n",
    "  read_html(\"https://catalog.registrar.ucla.edu/ucla-catalog18-19-4.html\") %>%\n",
    "  html_nodes(\".main-text\") %>%\n",
    "  html_nodes(\"li\") %>%\n",
    "  html_nodes(\"a\") %>%\n",
    "  html_text %>%\n",
    "  unlist %>%\n",
    "  str_to_lower %>%\n",
    "  str_squish() %>%\n",
    "  unique\n",
    "\n",
    "ucla_programs <- str_remove_all(ucla_programs,\n",
    "    regex(\"[:space:](ba|\n",
    "    bs|\n",
    "    ma|\n",
    "    ms|\n",
    "    phd|\n",
    "    llm, jd, sjd|\n",
    "    dnp|\n",
    "    md|\n",
    "    dds|\n",
    "    denv|\n",
    "    cphil)$\"\n",
    "  )\n",
    ") %>% \n",
    "  str_replace_all(\"—\", \"-\") %>% \n",
    "  iconv(to = \"UTF-8\")\n",
    "\n",
    "\n",
    "major_words <- c(elac_programs, ucla_programs)\n",
    "majors_dictionary <- dictionary(list(major = major_words))\n",
    "\n",
    "course_subjects <- c(job_words, major_words)\n",
    "course_subject_words <- types(tokens(course_subjects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_digits <- function(text) {\n",
    "  # It will be a lot easier to make regexes if  I convert all number words to digits first\n",
    "  text %>%\n",
    "    str_replace_all(pattern = regex(ignore_case = TRUE,\"\\\\bone\"), replacement = \"1\") %>%\n",
    "    str_replace_all(pattern = regex(ignore_case = TRUE,\"\\\\btwo\"), replacement = \"2\") %>%\n",
    "    str_replace_all(pattern = regex(ignore_case = TRUE,\"\\\\bthree\"), replacement = \"3\") %>%\n",
    "    str_replace_all(pattern = regex(ignore_case = TRUE,\"\\\\bfour\"), replacement = \"4\") %>%\n",
    "    str_replace_all(pattern = regex(ignore_case = TRUE,\"\\\\bfive\"), replacement = \"5\") %>%\n",
    "    str_replace_all(pattern = regex(ignore_case = TRUE,\"\\\\bsix\"), replacement = \"6\") %>%\n",
    "    str_replace_all(pattern = regex(ignore_case = TRUE,\"\\\\bseven\"), replacement = \"7\") %>%\n",
    "    str_replace_all(pattern = regex(ignore_case = TRUE,\"\\\\beight\"), replacement = \"8\") %>%\n",
    "    str_replace_all(pattern = regex(ignore_case = TRUE,\"\\\\bnine\"), replacement = \"9\") %>%\n",
    "    str_replace_all(pattern = regex(ignore_case = TRUE,\"\\\\bten\"), replacement = \"10\") %>%\n",
    "    str_replace_all(pattern = regex(ignore_case = TRUE,\"\\\\beleven\"), replacement = \"11\") %>%\n",
    "    str_replace_all(pattern = regex(ignore_case = TRUE,\"\\\\btwelve\"), replacement = \"12\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in stri_replace_all_regex(string, pattern, fix_replacement(replacement), :\n",
      "“argument is not an atomic vector; coercing”"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read titles and class codes from files\n",
    "# the filenames have more issues \n",
    "# e.g. (1).txt, (2).txt, ...,  _two_ spaces after the class code, no class code at all,\n",
    "# and also filenames with the word REV or Rev or Revised or Updated in them, followed by revision date\n",
    "# By the way, this might be evidence of the complexity of the task of writing the descriptions (to equitably get\n",
    "# qualified candidates?)\n",
    "job_titles <- map(file_list, read_lines, n_max = 1, skip_empty_rows = TRUE) %>% \n",
    "    str_squish %>% \n",
    "    str_replace_all(string = ., pattern = \" \", replacement = \"_\")\n",
    "\n",
    "class_codes  <-  map(file_list, read_lines, n_max = 10) %>%\n",
    "    map(.x = ., .f = grep, pattern = \"Class Code:\", value = TRUE, ignore.case = TRUE) %>% \n",
    "    str_squish \n",
    "\n",
    "titles_and_codes  <- paste0(job_titles, class_codes)\n",
    "\n",
    "\n",
    "# Arrange file contents into a dataframe so that tidytext can use them\n",
    "\n",
    "file_contents  <- map(file_list, read_file) %>% \n",
    "    set_names(x = ., nm = titles_and_codes) %>% \n",
    "    unlist\n",
    "\n",
    "# Perform some ad-hoc spellchecking\n",
    "file_contents2 <- data.frame(title = unlist(titles_and_codes), text = file_contents, filename = file_list) %>% \n",
    "  mutate(text = str_replace_all(text, pattern = \"\\r\\n([[:space:]])*(REQUIREMENT|REQUIREMENTS)([:space:])*\\r\\n\", \n",
    "    replacement = \"\\r\\nREQUIREMENTS/MINIMUM QUALIFICATION\\r\\n\") %>%\n",
    "    str_replace_all(pattern = \"/[[:space:]]\", replacement = \"/\") %>%\n",
    "    str_replace_all(pattern = \"[[:space:]]/\", replacement = \"/\") %>%\n",
    "    str_replace_all(pattern = \"MIMINUMUM|MINUMUM\", replacement = \"MINIMUM\") %>% \n",
    "    str_replace_all(pattern = \"QUALIFICAITON\", replacement = \"QUALIFICATIONS\") %>%  \n",
    "    str_replace_all(pattern = \"(\\r\\n)*([[:space:]])*REQUIREMENT/MINIMUM QUALIFICATION([[:space:]])*(\\r\\n)*\", replacement = \"\\r\\nREQUIREMENTS/MINIMUM QUALIFICATIONS\\r\\n\") %>% \n",
    "    str_replace_all(pattern = \"four year college\",\n",
    "        replacement = \"four-year college\") %>% \n",
    "    str_replace_all(pattern = \"two year college\", \n",
    "        replacement = \"two-year college\") \n",
    "    ) %>% \n",
    "mutate(text = str_squish(as.character(text)),\n",
    "  filename = as.character(filename),\n",
    "  title = as.character(title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating a single structured csv\n",
    "\n",
    "This is 30 or so little problems of tokenizing different aspects of a job bulletin\n",
    "\n",
    "1. Initial goal is  a prototype dataframe having (I assume) easiest-to-parse fields: FILE\\_NAME, JOB\\_CLASS\\_TITLE and \\_NO, OPEN\\_DATE, ...\n",
    "2. For each field, write a function to extract it from the 'text' field of the file_contents2 dataframe above that returns a vector of objects of admissible types from the data dictionary.\n",
    "3. Map these functions over job descriptions and bind them into a data frame, nesting within job titles\n",
    "4. Test the data frame for missing fields and allowable values\n",
    "5. Write to structured\\_descriptions.csv\n",
    "6. Iterate: try to find the best regexes for the job and clean ways to cut out junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_job_duties <- function(text) {\n",
    "  duties_match <-\n",
    "    str_extract(text, pattern = \"(duties[[:space:]])([[:print:]])*(requirements/minimum[[:space:]])\") %>%\n",
    "    str_remove(pattern = \"requirements/minimum\") %>%\n",
    "    str_remove(pattern = \"duties\") %>%\n",
    "    str_trim() %>%\n",
    "    str_to_sentence()\n",
    "  return(duties_match)\n",
    "}\n",
    "\n",
    "\n",
    "get_entry_salary <- function(text) {\n",
    "  # Define a salary regex\n",
    "  salary_regex <-\n",
    "    regex(pattern = \"(power is[[:space:]])?\\\\$([[:digit:]]){2,3},([[:digit:]]){3}(\\\\.[[:digit:]][[:digit:]])?(\\\\*)?([[:space:]])?(to \\\\$([[:digit:]]){2,3},([[:digit:]]){3}(\\\\.[[:digit:]][[:digit:]])?|(\\\\()?(flat)(\\\\s)?(-)?(\\\\s)?(rated)(\\\\)?)|and)\",\n",
    "      ignore_case = TRUE)\n",
    "  # Match all salary text\n",
    "  salary_matches <-\n",
    "    str_match_all(string = text, pattern = salary_regex)\n",
    "\n",
    "  # Identify whether general or dwp & clean text\n",
    "  salary_gen_text <- NA\n",
    "  salary_dwp_text <- NA\n",
    "  if (is_empty(salary_matches[[1]])) {\n",
    "    return(list(salary_gen = salary_gen_text, salary_dwp = salary_dwp_text))\n",
    "\n",
    "  } else {\n",
    "    # Also, there are a few jobs without salaries or only having DWP salaries\n",
    "    if ((sum(!is.na(salary_matches[[1]][, 1])) > 0) &\n",
    "        (is.na(salary_matches[[1]][1, 2]))) {\n",
    "      # logic to handle descriptions with no salary or DWP-salary only (airport police specialist, boilermaker and others)\n",
    "      salary_gen_text <- salary_matches[[1]][1, 1] %>%\n",
    "        str_replace_all(string = .,\n",
    "          pattern = \"\\\\$|,\",\n",
    "          replacement = \"\") %>%\n",
    "        str_replace(string = .,\n",
    "          pattern = \"[[:space:]]to[[:space:]]\",\n",
    "          replacement = \"-\") %>%\n",
    "        str_replace(string = .,\n",
    "          pattern = \"and\",\n",
    "          replacement = \"(flat-rated)\") %>%\n",
    "        str_squish()\n",
    "    }\n",
    "\n",
    "\n",
    "    if (sum(!is.na(salary_matches[[1]][, 2])) > 0) {\n",
    "      salary_dwp_text <- salary_matches[[1]] %>%\n",
    "        as_tibble(.name_repair = \"universal\") %>%\n",
    "        filter(!is.na(...2)) %>%\n",
    "        slice(1) %>%\n",
    "        str_replace_all(string = .,\n",
    "          pattern = \"\\\\$|,\",\n",
    "          replacement = \"\") %>%\n",
    "        str_replace(string = .,\n",
    "          pattern = \"[[:space:]]to[[:space:]]\",\n",
    "          replacement = \"-\") %>%\n",
    "        str_replace(\n",
    "          string = .,\n",
    "          pattern = regex(\n",
    "            ignore_case = TRUE,\n",
    "            \"power is\",\n",
    "            ignore.case = TRUE\n",
    "          ),\n",
    "          replacement = \"\"\n",
    "        ) %>%\n",
    "        str_squish() %>%\n",
    "        `[[`(1)\n",
    "    }\n",
    "\n",
    "    # test correct shape of returned object\n",
    "    return(list(salary_gen = salary_gen_text, salary_dwp = salary_dwp_text))\n",
    "  }\n",
    "} # end get salary\n",
    "\n",
    "\n",
    "get_open_date <- function(text) {\n",
    "  # Open date from file contents\n",
    "  open_date <-\n",
    "    (\n",
    "      str_trim(str_match(\n",
    "        string = text,\n",
    "        pattern = regex(\n",
    "          \"(open date)(:)*([:space:])+([:digit:]){2}(-)([:digit:]){2}(-)([:digit:]){2}\",\n",
    "          ignore_case = TRUE\n",
    "        )\n",
    "      )[1, 1]) %>%\n",
    "        str_match(string = ., pattern = \"([:digit:]){2}(-)([:digit:]){2}(-)([:digit:]){2}\")\n",
    "    )[1, 1]\n",
    "\n",
    "  test_date <-\n",
    "    try(expr = parse_date(open_date, format = \"%m-%d-%y\"),\n",
    "      silent = FALSE)\n",
    "  ## there are a bunch of these as well: just check first line against filename\n",
    "  error_flag <-\n",
    "    ifelse(!is.na(test_date), \"No Error\", \"Date Format Error\")\n",
    "  return(list(open_date = open_date, error_flag = error_flag))\n",
    "}\n",
    "\n",
    "# Get all requirement sets and subsets\n",
    "get_requirement_sets_and_details <- function(text) {\n",
    "  ## Function to return three pieces\n",
    "  ## Requirement sets and subsets\n",
    "  ## Process notes\n",
    "  ## Other text (ex: selective certifications for systems analyst)\n",
    "  req_s_and_d_regex <-\n",
    "    regex(ignore_case = TRUE, pattern = \"(requirements/minimum)([[:print:]])*(where to apply)\")\n",
    "\n",
    "  process_notes_regex <-\n",
    "    regex(ignore_case = TRUE, pattern = \"(process )? (notes)([[:print:]])*(where to apply)\")\n",
    "\n",
    "  req_matches <-\n",
    "    str_match_all(string = text, pattern = req_s_and_d_regex)\n",
    "  process_matches <-\n",
    "    str_match_all(string = text, pattern = process_notes_regex)\n",
    "\n",
    "  if ((!is_empty(process_matches[[1]])) &\n",
    "      (!is_empty(req_matches[[1]]))) {\n",
    "    req_matches[[1]][1, 1] <-\n",
    "      str_split(req_matches[[1]][1, 1], pattern = \"(process )? (notes)\")[[1]][1]\n",
    "  }\n",
    "\n",
    "  reqs_text <- NA\n",
    "  process_text <- NA\n",
    "\n",
    "  if (is_empty(req_matches[[1]])) {\n",
    "    return(list(reqs_text = reqs_text, process_text = process_text))\n",
    "\n",
    "  } else {\n",
    "    reqs_text <- req_matches[[1]][1, 1] %>%\n",
    "      str_remove_all(regex(\"where to apply\", ignore_case = TRUE)) %>%\n",
    "      str_remove_all(regex(\"requirements/minimum qualification.\", ignore_case = TRUE)) %>%\n",
    "      str_remove_all(regex(\"process\", ignore_case = TRUE)) %>%\n",
    "      str_squish()\n",
    "  }\n",
    "\n",
    "\n",
    "  if (is_empty(process_matches[[1]])) {\n",
    "    return(list(reqs_text = reqs_text, process_text = process_text))\n",
    "  } else {\n",
    "    process_text <- process_matches[[1]][1, 1] %>%\n",
    "      str_remove_all(regex(\"where to apply\", ignore_case = TRUE)) %>%\n",
    "      str_remove_all(regex(\"process notes\", ignore_case = TRUE)) %>%\n",
    "      str_squish()\n",
    "  }\n",
    "\n",
    "  return(list(reqs_text = reqs_text, process_text = process_text))\n",
    "} # end extract all requirements text\n",
    "\n",
    "# Get requirement sets and subsets based on enumerations/itemize line starts and conjunctions\n",
    "# This is actually a tree structure, but knowing that doesn't help\n",
    "# anything\n",
    "get_requirement_set <- function(text) {\n",
    "  split_req <- str_split(text, pattern = \"; or [[:digit:]]\")\n",
    "  reqset <- data.frame(split_req) %>% rownames_to_column()\n",
    "  colnames(reqset) <- c(\"REQUIREMENT_SET_ID\", \"REQUIREMENTS\")\n",
    "  reqset <-\n",
    "    mutate(reqset, REQUIREMENTS = as.character(REQUIREMENTS))\n",
    "  return(reqset)\n",
    "}\n",
    "\n",
    "get_requirement_subset <- function(text) {\n",
    "  split_sreq  <- str_split(text, pattern = \"; or [[:alpha:]]\\\\.\")\n",
    "  sreqset  <- data.frame(split_sreq) %>% rownames_to_column()\n",
    "  colnames(sreqset) <- c(\"REQUIREMENT_SUBSET_ID\", \"REQUIREMENTS\")\n",
    "  sreqset  <-\n",
    "    mutate(\n",
    "      sreqset,\n",
    "      REQUIREMENT_SUBSET_ID = as.character(REQUIREMENT_SUBSET_ID),\n",
    "      REQUIREMENTS = as.character(REQUIREMENTS)\n",
    "    ) %>%\n",
    "    mutate(\n",
    "      REQUIREMENTS = str_remove_all(REQUIREMENTS,\n",
    "        pattern = \"^([:digit:])?([:punct:])\") %>%\n",
    "        str_squish()\n",
    "    )\n",
    "  return(sreqset)\n",
    "}\n",
    "\n",
    "# Apply these functions to all requirement sets and subsets\n",
    "\n",
    "## Parse Education Requirements\n",
    "\n",
    "get_school_type <- function(text) {\n",
    "  school_type_dictionary <-\n",
    "    data.frame(\n",
    "      token = c(\n",
    "        \"4-year college\",\n",
    "        \"university\",\n",
    "        \"2-year college\",\n",
    "        \"community college\",\n",
    "        \"trade school\",\n",
    "        \"technical school\",\n",
    "        \"high school\",\n",
    "        \"g.e.d.\",\n",
    "        \"(police|lafd leadership) academy\",\n",
    "        \"art school\",\n",
    "        \"training program\",\n",
    "        \"chspe\",\n",
    "        \"military school\",\n",
    "        \"lineman college\",\n",
    "        \"lineman's college\"\n",
    "      ),\n",
    "      type = c(\n",
    "        \"college or university\",\n",
    "        \"college or university\",\n",
    "        \"2-year college\",\n",
    "        \"2-year college\",\n",
    "        \"technical training\",\n",
    "        \"technical training\",\n",
    "        \"secondary school\",\n",
    "        \"secondary school\",\n",
    "        \"technical training\",\n",
    "        \"technical training\",\n",
    "        \"technical training\",\n",
    "        \"secondary school\",\n",
    "        \"technical training\",\n",
    "        \"technical training\",\n",
    "        \"technical training\"\n",
    "      )\n",
    "    ) %>%\n",
    "    mutate_all(as.character)\n",
    "\n",
    "  school_types <-\n",
    "    regex(\n",
    "      ignore_case = TRUE,\n",
    "      pattern = paste(school_type_dictionary$token, collapse = \"|\")\n",
    "    )\n",
    "\n",
    "  school_matches <- str_match_all(text, pattern = school_types)\n",
    "  if (!is_empty(school_matches[[1]][, 1])) {\n",
    "    return(\n",
    "      school_type_dictionary %>%\n",
    "        filter(token %in% school_matches[[1]][, 1]) %>%\n",
    "        pull(type) %>%\n",
    "        unique %>%\n",
    "        paste(collapse = \" or \")\n",
    "    )\n",
    "  } else {\n",
    "    return(NA)\n",
    "  }\n",
    "} # end get school type\n",
    "\n",
    "get_education_major <- function(text) {\n",
    "  major_regex <-\n",
    "    regex(ignore_case = TRUE,\n",
    "      \"((degree|major) in)([:print:])*(and|from|\\\\.)\")\n",
    "  major_matches <- str_match_all(text, major_regex)\n",
    "  course_words_rx <-\n",
    "    regex(ignore_case = TRUE, paste(course_subject_words, collapse = \"|\"))\n",
    "\n",
    "  major_words   <-\n",
    "    str_extract_all(string = major_matches[[1]][, 1], course_words_rx) %>% unlist %>% paste(collapse = \" \")\n",
    "\n",
    "  if (!is_empty(major_matches[[1]][, 1])) {\n",
    "    return(\n",
    "      list(\n",
    "        major_matches = str_split(major_matches[[1]][1, 1], major_matches[[1]][1, 2])[[1]][2] %>%\n",
    "          str_split(pattern = major_matches[[1]][1, 5]) %>%\n",
    "          `[[`(1) %>%\n",
    "          `[`(1) %>%\n",
    "          str_squish(),\n",
    "        major_words = major_words\n",
    "      )\n",
    "    )\n",
    "  } else {\n",
    "    return(NA)\n",
    "  }\n",
    "} # end get major\n",
    "\n",
    "get_course_length <- function(text) {\n",
    "  course_len_regex <-\n",
    "    regex(ignore_case = TRUE,\n",
    "      \"[:digit:]{1,3} (quarter|semester|weeks) (unit)?(s)?\")\n",
    "  course_len_match <-\n",
    "    str_match_all(string = text, pattern = course_len_regex)\n",
    "\n",
    "  if (!is_empty(course_len_match[[1]][, 1])) {\n",
    "    return(course_len_match[[1]][, 1] %>%\n",
    "        paste(collapse = \",\") %>%\n",
    "        str_squish())\n",
    "  } else {\n",
    "    return(NA)\n",
    "  }\n",
    "}\n",
    "\n",
    "get_course_subject_and_count <- function(text) {\n",
    "  course_regex <-\n",
    "    regex(\n",
    "      ignore_case = TRUE,\n",
    "      \"(successful )?(completion (of|for)[:print:]*)?([:digit:]{0,3}|a)?([[:print:]-\\\\.]*)? (class|course|units|program)([[:print:]-\\\\.]*)(from|given|\\\\.)\"\n",
    "    )\n",
    "  course_matches <- str_match_all(text, course_regex)\n",
    "  course_words_rx <-\n",
    "    regex(ignore_case = TRUE, paste(course_subject_words, collapse = \"|\"))\n",
    "  course_words   <-\n",
    "    str_extract_all(string = course_matches[[1]][, 1], course_words_rx) %>% unlist %>% paste(collapse = \" \")\n",
    "\n",
    "  if (!is_empty(course_matches[[1]][, 8])) {\n",
    "    return(\n",
    "      list(\n",
    "        subject = course_matches[[1]][, 8] %>%\n",
    "          str_squish(),\n",
    "        count = course_matches[[1]][, 4],\n",
    "        course_words = course_words\n",
    "      )\n",
    "    )\n",
    "  } else {\n",
    "    return(NA)\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "## Parse Experience Requirements\n",
    "get_experience_length <- function(text) {\n",
    "  experience_regex <-\n",
    "    regex(ignore_case = TRUE, pattern = \"([[:digit:]])+ (years|months) of (full|part)(-)?time (paid|volunteer) experience ([[:print:]])*\\\\.\")\n",
    "  experience_matches <-\n",
    "    str_match_all(text, pattern = experience_regex)\n",
    "  return(experience_matches)\n",
    "}\n",
    "\n",
    "get_exp_job_class_title <- function(text) {\n",
    "  job_title_sentence <-\n",
    "    regex(\n",
    "      ignore_case = TRUE,\n",
    "      \"(experience[:print:]*)((as a)|(at th(e|at) level))([:print:]*)(;|\\\\.)\"\n",
    "    )\n",
    "  job_words_rx <-\n",
    "    regex(ignore_case = TRUE, paste(job_words, collapse = \"|\"))\n",
    "  job_function_regex <-\n",
    "    regex(ignore_case = TRUE, \"([:print:]*ing)([:print:]*)(;|\\\\.)\")\n",
    "\n",
    "  job_sentences <- str_match_all(text, job_title_sentence)\n",
    "  exp_job_words <-\n",
    "    str_extract_all(string = job_sentences[[1]][, 1], job_words_rx) %>% unlist %>% paste(collapse = \" \")\n",
    "  job_functions <-\n",
    "    str_extract_all(string = job_sentences[[1]][, 1], pattern = job_function_regex)  %>% unlist %>% paste(collapse = \" \")\n",
    "\n",
    "  ## TODO: add get alternative responsibilities\n",
    "\n",
    "  if (!is_empty(job_sentences[[1]][, 7])) {\n",
    "    return(\n",
    "\n",
    "        exp_job_title = job_sentences[[1]][,7]\n",
    "\n",
    "    )\n",
    "  } else {\n",
    "    return(NA)\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Drivers' licences\n",
    "get_drivers_license_req <- function(text) {\n",
    "  dl_regex <-\n",
    "    regex(ignore_case = TRUE, pattern = \"(valid california class)([:print:]*) (driver(')?(s)? licen(s|c)e)\")\n",
    "  dl_matches <- str_match_all(text, pattern = dl_regex)\n",
    "\n",
    "\n",
    "  if (!is_empty(dl_matches[[1]][, 3])) {\n",
    "    return(list(\n",
    "      required_dl_class = dl_matches[[1]][1, 3] %>% str_remove_all(regex(\"class\", ignore_case = TRUE)) %>% str_squish()\n",
    "    ))\n",
    "  } else{\n",
    "    return(NA)\n",
    "  }\n",
    "\n",
    "}\n",
    "\n",
    "## Parse examination requirement (not contained in requirements)\n",
    "get_exam_type <- function(text) {\n",
    "  exam_regex <-\n",
    "    regex(ignore_case = TRUE,\n",
    "      \"(this examination is to be given)([:print:]*)(the city of los angeles)\")\n",
    "  str_match(text, exam_regex)[[3]] %>%\n",
    "    str_squish() %>%\n",
    "    str_extract_all(pattern = \"(open (competitive)?)|((interdepartmental)? promotional)\") %>%\n",
    "    unlist %>%\n",
    "    paste(collapse = \", \") %>%\n",
    "    return()\n",
    "}\n",
    "\n",
    "\n",
    "get_exp_job_class_function <- function(text){\n",
    "\n",
    "  experience_fn_regex <-\n",
    "    regex(ignore_case = TRUE, pattern = \"(experience in)([[:print:]])*(\\\\.|;)\")\n",
    "  experience_matches <-\n",
    "    str_match(text, pattern = experience_fn_regex)\n",
    "   return(experience_matches)\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating a structured csv\n",
    "- Using the ideal of a job bulletin as a tree and one line (leaves of depth >1) or one column (leaves of depth 1) per leaf on the tree, I nest within job bulletin titles and generate a few rows for each one; then I bind rows together.\n",
    "- I wrote one function for each set of related columns, so I generate one dataframe for each set of such columns\n",
    "- After that, I left-join all of these little dataframes into one big one which I plan to output to csv.\n",
    "- There's still a lot of garbage in the big dataframe, so I need to define tests on it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Joining, by = c(\"title\", \"REQUIREMENT_SET_ID\", \"REQUIREMENT_SUBSET_ID\")\n",
      "Joining, by = \"title\"\n",
      "Joining, by = \"title\"\n",
      "Joining, by = \"title\"\n",
      "Joining, by = c(\"title\", \"REQUIREMENT_SET_ID\", \"REQUIREMENT_SUBSET_ID\")\n",
      "Joining, by = c(\"title\", \"REQUIREMENT_SET_ID\", \"required_dl_class\")\n",
      "Joining, by = c(\"title\", \"REQUIREMENT_SET_ID\", \"REQUIREMENT_SUBSET_ID\")\n",
      "Joining, by = c(\"title\", \"REQUIREMENT_SET_ID\", \"REQUIREMENT_SUBSET_ID\")\n",
      "Joining, by = c(\"title\", \"REQUIREMENT_SET_ID\", \"REQUIREMENT_SUBSET_ID\")\n",
      "Joining, by = c(\"title\", \"REQUIREMENT_SET_ID\", \"REQUIREMENT_SUBSET_ID\")\n",
      "Warning message:\n",
      "“Expected 2 pieces. Missing pieces filled with `NA` in 3 rows [322, 634, 635].”"
     ]
    }
   ],
   "source": [
    "open_dates <- file_contents2 %>%\n",
    "  group_by(title) %>%\n",
    "  do(.data = .,\n",
    "    data.frame(get_open_date(.$text)) %>%\n",
    "      mutate_if(is.factor, as.character)) %>%\n",
    "  ungroup\n",
    "\n",
    "suppressMessages(\n",
    "  salaries <- file_contents2 %>%\n",
    "    group_by(title) %>%\n",
    "    do(\n",
    "      .data = .,\n",
    "      data.frame(get_entry_salary(.$text)) %>%\n",
    "        mutate_if(is.factor, as.character)\n",
    "    ) %>%\n",
    "    ungroup\n",
    ")\n",
    "\n",
    "duties <- file_contents2 %>%\n",
    "  group_by(title) %>%\n",
    "  do(.data = .,\n",
    "    data.frame(get_job_duties(.$text)) %>%\n",
    "      mutate_if(is.factor, as.character)) %>%\n",
    "  ungroup\n",
    "requirements_and_details_to_parse <-\n",
    "  file_contents2 %>%\n",
    "  group_by(title) %>%\n",
    "  do(\n",
    "    .data = .,\n",
    "    data.frame(get_requirement_sets_and_details(.$text)) %>% mutate_if(is.factor, as.character)\n",
    "  ) %>%\n",
    "  ungroup\n",
    "\n",
    "\n",
    "requirement_sets <- requirements_and_details_to_parse %>%\n",
    "  group_by(title) %>%\n",
    "  do(.data = .,\n",
    "    data.frame(get_requirement_set(.$reqs_text)) %>% mutate_if(is.factor, as.character)) %>%\n",
    "  ungroup()\n",
    "\n",
    "\n",
    "requirement_subsets <- requirement_sets %>%\n",
    "  group_by(title, REQUIREMENT_SET_ID) %>%\n",
    "  do(.data = .,\n",
    "    data.frame(get_requirement_subset(.$REQUIREMENTS)) %>% mutate_if(is.factor, as.character)) %>%\n",
    "  ungroup %>%\n",
    "  mutate(\n",
    "    REQUIREMENT_SUBSET_ID = recode(\n",
    "      REQUIREMENT_SUBSET_ID,\n",
    "      `1` = \"A\",\n",
    "      `2` = \"B\",\n",
    "      `3` = \"C\",\n",
    "      `4` = \"D\",\n",
    "      `5` = \"E\",\n",
    "      `6` = \"F\",\n",
    "      `7` = \"G\",\n",
    "      `8` = \"H\",\n",
    "      `9` = \"I\",\n",
    "      `10` = \"J\",\n",
    "      `11` = \"K\",\n",
    "      `12` = \"L\",\n",
    "      `13` = \"M\",\n",
    "      `14` = \"N\",\n",
    "      `15` = \"O\",\n",
    "      `16` = \"P\",\n",
    "      `17` = \"Q\",\n",
    "      `18` = \"R\",\n",
    "      `19` = \"S\",\n",
    "      `20` = \"T\",\n",
    "      `21` = \"U\",\n",
    "      `22` = \"V\",\n",
    "      `23` = \"W\",\n",
    "      `24` = \"X\",\n",
    "      `25` = \"Y\",\n",
    "      `26` = \"Z\",\n",
    "      `27` = \"AA\",\n",
    "      `28` = \"AB\",\n",
    "      `29` = \"AC\",\n",
    "      `30` = \"AD\",\n",
    "      `31` = \"AE\",\n",
    "      `32` = \"AF\",\n",
    "      `33` = \"AH\",\n",
    "      `34` = \"AI\",\n",
    "      `35` = \"AJ\",\n",
    "      `36` = \"AK\"\n",
    "    ),\n",
    "    REQUIREMENTS = words_to_digits(REQUIREMENTS)\n",
    "  )\n",
    "\n",
    "process_notes <- requirements_and_details_to_parse %>%\n",
    "  group_by(title) %>%\n",
    "  do(\n",
    "    .data = .,\n",
    "    data.frame(get_requirement_set(.$process_text)) %>% mutate_if(is.factor, as.character) %>%\n",
    "      rename(process_notes = 'REQUIREMENTS')\n",
    "  ) %>%\n",
    "  ungroup\n",
    "\n",
    "experience_subset <- requirement_subsets %>%\n",
    "  group_by(title, REQUIREMENT_SET_ID, REQUIREMENT_SUBSET_ID) %>%\n",
    "  do(\n",
    "    .data = .,\n",
    "    data.frame(get_experience_length(.$REQUIREMENTS))  %>% mutate_if(is.factor, as.character) %>%\n",
    "      rename(\n",
    "        experience = X1,\n",
    "        length = X2,\n",
    "        unit = X3,\n",
    "        full_time_part_time = X4,\n",
    "        drop1 = X5,\n",
    "        paid_volunteer = X6,\n",
    "        drop2 = X7\n",
    "      )\n",
    "  ) %>%\n",
    "  select(-starts_with(\"drop\")) %>%\n",
    "  ungroup\n",
    "\n",
    "experience_process <- process_notes %>%\n",
    "  group_by(title,  REQUIREMENT_SET_ID) %>%\n",
    "  do(\n",
    "    .data = .,\n",
    "    data.frame(get_experience_length(.$process_notes))  %>% mutate_if(is.factor, as.character) %>%\n",
    "      rename(\n",
    "        experience = X1,\n",
    "        length = X2,\n",
    "        unit = X3,\n",
    "        full_time_part_time = X4,\n",
    "        drop1 = X5,\n",
    "        paid_volunteer = X6,\n",
    "        drop2 = X7\n",
    "      )\n",
    "  ) %>%\n",
    "  select(-starts_with(\"drop\")) %>%\n",
    "  ungroup\n",
    "\n",
    "dl_subset <- requirement_subsets %>%\n",
    "  group_by(title, REQUIREMENT_SET_ID, REQUIREMENT_SUBSET_ID) %>%\n",
    "  do(\n",
    "    .data = .,\n",
    "    data.frame(get_drivers_license_req(.$REQUIREMENTS))  %>% mutate_if(is.factor, as.character)\n",
    "  ) %>%\n",
    "  ungroup\n",
    "\n",
    "dl_process <- process_notes %>%\n",
    "  group_by(title,  REQUIREMENT_SET_ID) %>%\n",
    "  do(\n",
    "    .data = .,\n",
    "    data.frame(get_drivers_license_req(.$process_notes))  %>% mutate_if(is.factor, as.character)\n",
    "\n",
    "  ) %>%\n",
    "  ungroup\n",
    "\n",
    "school_type <- requirement_subsets %>%\n",
    "  group_by(title, REQUIREMENT_SET_ID, REQUIREMENT_SUBSET_ID) %>%\n",
    "  do(.data = .,\n",
    "    data.frame(get_school_type(.$REQUIREMENTS))  %>% mutate_if(is.factor, as.character)) %>%\n",
    "  ungroup\n",
    "\n",
    "major <- requirement_subsets %>%\n",
    "  group_by(title, REQUIREMENT_SET_ID, REQUIREMENT_SUBSET_ID) %>%\n",
    "  do(.data = .,\n",
    "    data.frame(get_education_major(.$REQUIREMENTS))  %>% mutate_if(is.factor, as.character)) %>%\n",
    "  ungroup\n",
    "\n",
    "course_lengths <- requirement_subsets %>%\n",
    "  group_by(title, REQUIREMENT_SET_ID, REQUIREMENT_SUBSET_ID) %>%\n",
    "  do(\n",
    "    .data = .,\n",
    "    data.frame(course_length = get_course_length(.$REQUIREMENTS))  %>% mutate_if(is.factor, as.character)\n",
    "  ) %>%\n",
    "  ungroup\n",
    "\n",
    "\n",
    "course_subject_and_count <- requirement_subsets %>%\n",
    "  group_by(title, REQUIREMENT_SET_ID, REQUIREMENT_SUBSET_ID) %>%\n",
    "  do(\n",
    "    .data = .,\n",
    "    data.frame(get_course_subject_and_count(.$REQUIREMENTS))  %>% mutate_if(is.factor, as.character)\n",
    "  ) %>%\n",
    "  ungroup\n",
    "\n",
    "job_title_subset <- requirement_subsets %>%\n",
    "  group_by(title, REQUIREMENT_SET_ID, REQUIREMENT_SUBSET_ID) %>%\n",
    "  do(\n",
    "    .data = .,\n",
    "    data.frame(get_exp_job_class_title(.$REQUIREMENTS)) %>%\n",
    "      mutate_if(is.factor, as.character)\n",
    "  ) %>%\n",
    "  ungroup\n",
    "\n",
    "job_class_function <- requirement_subsets %>%\n",
    "  group_by(title, REQUIREMENT_SET_ID, REQUIREMENT_SUBSET_ID) %>%\n",
    "  do(\n",
    "    .data = .,\n",
    "    exp_job_class_function = data.frame(get_exp_job_class_function(.$REQUIREMENTS)) %>%\n",
    "      mutate_if(is.factor, as.character)\n",
    "  ) %>%\n",
    "  ungroup\n",
    "\n",
    "examinations <- file_contents2 %>%\n",
    "  group_by(title) %>%\n",
    "  mutate(EXAM_TYPE = get_exam_type(text)) %>%\n",
    "  ungroup\n",
    "\n",
    "\n",
    "nonfunctional_prototype_csv <-\n",
    "  left_join(requirement_subsets, experience_subset) %>%\n",
    "  left_join(open_dates) %>%\n",
    "  left_join(duties) %>%\n",
    "  left_join(salaries) %>%\n",
    "  left_join(dl_subset) %>%\n",
    "  left_join(dl_process) %>%\n",
    "  # left_join(lic_subset) %>%\n",
    "  # left_join(lic_process) %>% delete these functions!\n",
    "  left_join(school_type) %>%\n",
    "  ## left_join(major) %>%  delete this function\n",
    "  # left_join(course_subject_and_count) %>%\n",
    "  left_join(course_lengths) %>%\n",
    "  left_join(job_title_subset) %>%\n",
    "  left_join(job_class_function) %>%\n",
    "  separate(title,\n",
    "    into = c('job_title', 'class_code'),\n",
    "    sep = \"Class Code: \") %>%\n",
    "  mutate(\n",
    "    education_years = str_replace_all(\n",
    "      get_school_type...REQUIREMENTS.,\n",
    "      \"postgraduate college or university\",\n",
    "      \"6+\"\n",
    "    ) %>%\n",
    "      str_replace_all(\"college or university\", \"4\") %>%\n",
    "      str_replace_all(\"2-year college\", \"2\") %>%\n",
    "      str_remove_all(regex(ignore_case = TRUE, \"(or )?technical training\")) %>%\n",
    "      str_remove_all(regex(ignore_case = TRUE, \"(or )?secondary school\")) %>%\n",
    "      str_squish()\n",
    "  ) %>%\n",
    "  unite(col = \"experience_length\", length, unit, sep = \" \")\n",
    "\n",
    "\n",
    "# build a csv by keyword lookup in context ----\n",
    "experience_job_class_title <-\n",
    "  kwic(nonfunctional_prototype_csv$REQUIREMENTS,\n",
    "    pattern = job_dictionary) %>%\n",
    "  filter(!grepl(\"license\", x = pre),\n",
    "    !grepl(\"license\", x = post),\n",
    "    !grepl(\"senior\", x = pre)) %>%\n",
    "  select(docname, keyword) %>%\n",
    "  mutate(doc_row = as.integer(str_remove_all(docname, \"text\"))) %>%\n",
    "  group_by(docname, doc_row) %>%\n",
    "  summarize(class_titles = paste(unique(as.character(keyword)), collapse = \", \"))\n",
    "\n",
    "nonfunctional_prototype_csv$exp_job_class_title <- NA\n",
    "nonfunctional_prototype_csv$exp_job_class_title[experience_job_class_title$doc_row] <-\n",
    "  experience_job_class_title$class_titles\n",
    "\n",
    "ed_major <- kwic(nonfunctional_prototype_csv$REQUIREMENTS,\n",
    "  pattern = majors_dictionary,\n",
    "  window = 36) %>%\n",
    "  filter(grepl(\"degree|graduation\", x = pre)) %>%\n",
    "  select(docname, keyword) %>%\n",
    "  mutate(doc_row = as.integer(str_remove_all(docname, \"text\"))) %>%\n",
    "  group_by(docname, doc_row) %>%\n",
    "  summarize(class_titles = paste(unique(as.character(keyword)), collapse = \", \"))\n",
    "\n",
    "nonfunctional_prototype_csv$education_major <- NA\n",
    "nonfunctional_prototype_csv$education_major[ed_major$doc_row] <-\n",
    "  ed_major$class_titles\n",
    "\n",
    "addtl_license <-\n",
    "  kwic(\n",
    "    nonfunctional_prototype_csv$REQUIREMENTS,\n",
    "    pattern = c(\n",
    "      \"license\",\n",
    "      \"licensed\",\n",
    "      \"certificate\",\n",
    "      \"certified\",\n",
    "      \"cert\",\n",
    "      \"certs\",\n",
    "      \"certification\"\n",
    "    ),\n",
    "    window = 24\n",
    "  ) %>%\n",
    "  filter(!grepl(\"driver\", x = pre)) %>%\n",
    "  select(docname, pre, keyword, post) %>%\n",
    "  mutate(\n",
    "    doc_row = as.integer(str_remove_all(docname, \"text\")),\n",
    "    pre = str_extract(\n",
    "      pre,\n",
    "      pattern = regex(\n",
    "        ignore_case = TRUE,\n",
    "        \"(must|(;[:space:])?and|[:upper:]|[:punct:])([[:print:]]*)$\"\n",
    "      )\n",
    "    ),\n",
    "    post = str_extract(\n",
    "      post,\n",
    "      pattern = regex(\n",
    "        ignore_case = TRUE,\n",
    "        \"^[[:print:]]*(\\\\band\\\\b|[[:punct:]]|required)\"\n",
    "      )\n",
    "    )\n",
    "  ) %>%\n",
    "  unite(pre,\n",
    "    keyword,\n",
    "    post,\n",
    "    col = \"add_lic\",\n",
    "    sep = \" \",\n",
    "    remove = FALSE) %>%\n",
    "  group_by(docname, doc_row) %>%\n",
    "  summarize(\n",
    "    additional_license = paste(unique(as.character(add_lic)), collapse = \" \"),\n",
    "    adpost = paste(unique(as.character(post)), collapse = \" \")\n",
    "  ) %>%\n",
    "  mutate(additional_license = paste(types(\n",
    "    tokens_remove(\n",
    "    tokens(additional_license),\n",
    "      stopwords())), collapse = \" \")) %>%\n",
    "  ungroup\n",
    "#\n",
    "nonfunctional_prototype_csv$addtl_lic <- NA\n",
    "nonfunctional_prototype_csv$addtl_lic[addtl_license$doc_row] <-\n",
    "  addtl_license$additional_license\n",
    "#\n",
    "# nonfunctional_prototype_csv$addl_post <- NA\n",
    "# nonfunctional_prototype_csv$addl_post[addtl_license$doc_row] <- addtl_license$adpost\n",
    "\n",
    "addtl_license_process_notes <-\n",
    "  kwic(\n",
    "    process_notes$process_notes,\n",
    "    pattern = c(\n",
    "      \"license\",\n",
    "      \"licensed\",\n",
    "      \"certificate\",\n",
    "      \"certified\",\n",
    "      \"cert\",\n",
    "      \"certs\",\n",
    "      \"certification\"\n",
    "    ),\n",
    "    window = 24\n",
    "  ) %>%\n",
    "  filter(!grepl(\"driver\", x = pre)) %>%\n",
    "  select(docname, pre, keyword, post) %>%\n",
    "  mutate(\n",
    "    doc_row = as.integer(str_remove_all(docname, \"text\")),\n",
    "    pre = str_extract(\n",
    "      pre,\n",
    "      pattern = regex(\n",
    "        ignore_case = TRUE,\n",
    "        \"(must|(;[:space:])?and|[:upper:]|[:punct:])([[:print:]]*)$\"\n",
    "      )\n",
    "    ),\n",
    "    post = str_extract(\n",
    "      post,\n",
    "      pattern = regex(\n",
    "        ignore_case = TRUE,\n",
    "        \"^[[:print:]]*(\\\\band\\\\b|[[:punct:]]|required)\"\n",
    "      )\n",
    "    )\n",
    "  ) %>%\n",
    "  unite(pre,\n",
    "    keyword,\n",
    "    post,\n",
    "    col = \"add_lic\",\n",
    "    sep = \" \",\n",
    "    remove = FALSE) %>%\n",
    "  group_by(docname, doc_row) %>%\n",
    "  summarize(\n",
    "    additional_license = paste(unique(as.character(add_lic)), collapse = \" \"),\n",
    "    adpost = paste(unique(as.character(post)), collapse = \" \")\n",
    "  ) %>%\n",
    "  mutate(additional_license = paste(\n",
    "    types(\n",
    "      tokens_remove(\n",
    "        tokens(additional_license),\n",
    "        stopwords())), collapse = \" \")) %>%\n",
    "  ungroup\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and output to CSV\n",
    "Important facts: \n",
    "1. Not every job has every kind of requirement\n",
    "2. Not every requirement is formatted in the same way\n",
    "\n",
    "Tests\n",
    "1. Does every job listing have at least 1 experience or education requirement?\n",
    "2. Do all jobs with 'driver' in the title require licenses?\n",
    "3. Are college majors represented as words/short phrases?\n",
    "4. Are course subjects represented as words/short phrases?\n",
    "5. Does every course requirement have a matching course count?\n",
    "6. Does every education requirement have a matching education length? Must it?\n",
    "7. Are required job ranks (e.g. carpenter's _aide_) being captured?\n",
    "8. Are any fields longer than 140 characters?\n",
    "9. Does every line represent a subrequirement that is preceded by an _or_ in the bulletin? Subrequirements conjoined with _and_ should be on the same line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing code\n",
    "## Not formally testing yet\n",
    "\n",
    "# Any jobs missing ed & exp requirements?\n",
    "  # Unlicensed \"drivers\"?\n",
    "# Longest descriptions\n",
    "# Does every course requirement have a matching course count?\n",
    "# Does every education requirement have a matching education length? Must it?\n",
    "# Are required job ranks (e.g. carpenter's aide) being captured?\n",
    "# Does every line represent a subrequirement that is preceded by an or in the bulletin? Subrequirements conjoined with and should be on the same line.\n",
    "\n",
    "\n",
    "export_csv <- nonfunctional_prototype_csv %>%\n",
    "  select(-contains(\"...\"), -error_flag, -exp_job_class_function)\n",
    "\n",
    "write_csv(export_csv, path = \"ds4gLA_structured_bulletins.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsed with column specification:\n",
      "cols(\n",
      "  `Field Name` = \u001b[31mcol_character()\u001b[39m,\n",
      "  `Annotation Letter` = \u001b[31mcol_character()\u001b[39m,\n",
      "  Description = \u001b[31mcol_character()\u001b[39m,\n",
      "  `Data Type` = \u001b[31mcol_character()\u001b[39m,\n",
      "  `Allowable Values` = \u001b[31mcol_character()\u001b[39m,\n",
      "  `Accepts Null Values?` = \u001b[31mcol_character()\u001b[39m,\n",
      "  `Additional Notes` = \u001b[31mcol_character()\u001b[39m\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Generate data dictionary (use clean_names() first!)\n",
    "given_data_dictionary  <- read_csv(\"../input/data-science-for-good-city-of-los-angeles/cityofla/CityofLA/Additional data/kaggle_data_dictionary.csv\")\n",
    "output_data_dictionary   <- data.frame(field_name = NULL, # colnames(nonfunctional_prototype_csv),\n",
    "                                annotation_letter = NULL,\n",
    "                                description = NULL,\n",
    "                                data_type = NULL,\n",
    "                                allowable_values = NULL,\n",
    "                                accepts_null = NULL,\n",
    "                                additional_notes = NULL\n",
    "                               )\n",
    "\n",
    "\n",
    "## Add custom columns and descriptions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
